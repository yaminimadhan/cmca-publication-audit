{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e856d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(\"..\") / \"src\"))\n",
    "#Retrival Logic\n",
    "from similarity_search import search_sentences\n",
    "#Find mentions of staff and instruments\n",
    "#from match import (staff_list, staff_mention, instrument_list, instrument_mention)\n",
    "#Extract the text from the pdf\n",
    "#from extraction import extract_sangram\n",
    "#extract_sangram.extract_pdf(\"data/raw_pdfs/27_Negative.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load extracted sentence\n",
    "with open(\"../outputs/processed_text/27_Negative.sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = f.read().splitlines()\n",
    "\n",
    "#Staffs\n",
    "staff_list = staff_list(\"staffs.txt\")\n",
    "staff_name = staff_mention(sentences, staff_list)\n",
    "\n",
    "#Instruments\n",
    "instrument_list = instrument_list(\"instruments.txt\")\n",
    "instruments = instrument_mention(sentences, instrument_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(\"../outputs/processed_text/27_Negative.sentences.txt\")\n",
    "sentences = [f\"passage: {line.strip()}\" for line in input_path.read_text(encoding=\"utf-8\").splitlines() if line.strip()] #Convert the sentences in the format that model expects\n",
    "\n",
    "#Call the retrival logic\n",
    "results = search_sentences(sentences, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba1efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Storing Gold_Standard text in Database - Dont uncomment this \n",
    "#import store_embedding\n",
    "\n",
    "\n",
    "#Thresshold the similarity score\n",
    "threshold = 0.70 #Changed the logic so can reduce as well\n",
    "filtered_results = {\n",
    "    query: [(match, score) for (match, score) in matches if score >= threshold]\n",
    "    for query, matches in results.items()\n",
    "    if any(score >= threshold for (_, score) in matches)\n",
    "}\n",
    "#Number of extracted query sentences that have at least one match above thresshold in the Gold Standards\n",
    "matched_sentence = sum(1 for matches in filtered_results.values() if matches)\n",
    "print(matched_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build list of queries with max similarity\n",
    "ranked_queries = [\n",
    "    (query_sentence, matches, max(score for _, score in matches))\n",
    "    for query_sentence, matches in filtered_results.items()\n",
    "]\n",
    "\n",
    "#Sort queries by their best score\n",
    "ranked_queries.sort(key=lambda triple: triple[2], reverse=True)\n",
    "\n",
    "#Print\n",
    "print(\"The follwoing sentences matched the query with similarity above the threshold. Below i have listed the matched sentences(passage) with query they match to and their similarity score.\")\n",
    "for query_sentence, matches, _ in ranked_queries:\n",
    "    print(f\"\\n{query_sentence}\")\n",
    "    for matched_sentence, similarity_score in matches:\n",
    "        print(f\"{matched_sentence} (sim={similarity_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75247faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take queries sorted by their best similarity score\n",
    "best_queries = [query for query, _, _ in ranked_queries[:7]]\n",
    "#Checking\n",
    "print(\"Queries to send to LLM:\")\n",
    "for query in best_queries:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb29a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query_sentence, matches):\n",
    "    top_match, top_score = matches[0] #There are 3, but here just for the context passing only top one. \n",
    "    bullet_matches = f'- \"{top_match}\" (Similarity: {top_score:.2f})'\n",
    "\n",
    "    return f\"\"\"\n",
    "You are acting as a **research compliance auditor**. \n",
    "Your role is to verify whether a given sentence from a scientific document constitutes a **formal acknowledgement** of institutional or financial support.\n",
    "\n",
    "The sentence to review is:\n",
    "\n",
    "**Extracted Sentence:**\n",
    "\"{query_sentence.replace(\"passage: \", \"\")}\"\n",
    "\n",
    "This sentence was selected because it closely matches known acknowledgement phrases:\n",
    "{bullet_matches}\n",
    "\n",
    "### Your task\n",
    "Determine whether the sentence contains a **formal acknowledgement** that refers specifically to **one or more of the following known entities**:\n",
    "- CMCA (Centre for Microscopy Characterisation and Analysis)\n",
    "- The University of Western Australia (UWA)\n",
    "- Microscopy Australia or its nodes\n",
    "- NCRIS(National Collaborative Research Infrastructure Strategy) research infrastructure programs\n",
    "\n",
    "### Decision Criteria\n",
    "Classify the sentence as a **formal acknowledgement** only if it clearly refers to:\n",
    "- Use or access of CMCA, UWA microscopy facilities, or Microscopy Australia\n",
    "- Technical or analytical assistance provided by these institutions\n",
    "- support by NCRIS funding\n",
    "\n",
    "Do **not** classify it as an acknowledgement if it:\n",
    "- Only thanks individuals without institutional affiliation\n",
    "- Provides generic gratitude with no clear link to facilities, funding, or institutional support\n",
    "\n",
    "### Respond in this exact format:\n",
    "Answer: [Yes or No]  \n",
    "Reason: Explain **why** this sentence qualifies (or not) based on the decision criteria above. Use 1-3 sentences only.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5182e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "#Initialize client\n",
    "client = OpenAI(api_key= \"API KEY\")\n",
    "\n",
    "def verify_acknowledgement(ranked_queries, top_k = 7):\n",
    "    results = {}\n",
    "    top_queries = ranked_queries[:top_k]\n",
    "\n",
    "    for query_sentence, matches, _ in top_queries:\n",
    "        #Build the prompt\n",
    "        prompt = build_prompt(query_sentence, matches)\n",
    "        #Send to LLM\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a research auditor.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        #Save response\n",
    "        results[query_sentence] = response.choices[0].message.content.strip()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and show the result\n",
    "result = verify_acknowledgement(ranked_queries, top_k = 7)\n",
    "for query_sentence, response in result.items():\n",
    "    print(f\"Query: {query_sentence}\")\n",
    "    print(f\"LLM Response:\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85202a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If one Yes - then document is qualified\n",
    "#Decision \n",
    "def decision(responses):\n",
    "    for resp in responses.values():\n",
    "        if \"Answer: Yes\" in resp:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#Apply decision\n",
    "if decision(result):\n",
    "    print(\"There is a formal acknowledgement in the document\")\n",
    "else:\n",
    "    print(\"There is no formal acknowledgement in the document\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c32476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#Apply decision\n",
    "document_status = \"Yes\" if decision(result) else \"No\"\n",
    "\n",
    "#Build JSON structure\n",
    "output_json = {\n",
    "    \"Acknowledgement\": document_status,\n",
    "    \"Sentence_verifications\": [\n",
    "        {\n",
    "            \"query\": query_sentence,\n",
    "            \"llm_response\": response,\n",
    "            \"answer\": \"Yes\" if \"Answer: Yes\" in response else \"No\"\n",
    "        }\n",
    "        for query_sentence, response in result.items()\n",
    "    ], \n",
    "    \"CMCA Staff\": staff_name,\n",
    "    \"Instrument\": instruments\n",
    "}\n",
    "\n",
    "#Save to JSON file\n",
    "with open(\"result.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_json, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (capstoneProject)",
   "language": "python",
   "name": "capstoneproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
