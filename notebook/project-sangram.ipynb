{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62989046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(\"..\") / \"src\"))\n",
    "\n",
    "#Retrival Logic\n",
    "from similarity_search import search_sentences\n",
    "#Extract the text from the pdf\n",
    "from extraction import extract_sangram\n",
    "extract_sangram.extract_pdf(\"data/raw_pdfs/sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the similarity search function - Does Query Embedding + Search in one call\n",
    "\n",
    "#Extracted sentences - (these two lines below can be kept together in similarity_search.py) - So modal loading and stuffs all will be there\n",
    "input_path = Path(\"../outputs/processed_text/sample.sentences.txt\")\n",
    "sentences = [f\"passage: {line.strip()}\" for line in input_path.read_text(encoding=\"utf-8\").splitlines() if line.strip()] #Convert the sentences in the format that model expects\n",
    "\n",
    "#Call the retrival logic\n",
    "results = search_sentences(sentences, k=20) #K is 30 by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba1efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Storing Gold_Standard text in Database - Dont uncomment this \n",
    "#import store_embedding\n",
    "\n",
    "\n",
    "#Thresshold the similarity score - 85%(might miss few samples)\n",
    "threshold = 0.80#This number should be perfectly tuned i started from 80 where in 80 i found many false matches so increased the thresshold to 85 which seems good.\n",
    "filtered_results = {\n",
    "    q: [(m, s) for (m, s) in matches if s >= threshold]\n",
    "    for q, matches in results.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of extracted query sentences that have at least one match above 85 in the Gold Standards\n",
    "matched_sentence = sum(1 for matches in filtered_results.values() if matches)\n",
    "print(matched_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c4705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the result\n",
    "for query, matches in filtered_results.items():\n",
    "    if matches:\n",
    "        print(f\"\\n Query: {query}\")\n",
    "        for match, score in matches:\n",
    "            print(f\"   → {match} (sim={score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75247faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through queries that have strong matches\n",
    "for query, matches in filtered_results.items():\n",
    "    if matches:\n",
    "        print(\" Queries to send to LLM:\\n\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369bf61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query_sentence, matches):\n",
    "    #Keep only 5 highest similar from gold standard\n",
    "    top_matches = matches[:5]  \n",
    "\n",
    "    bullet_matches = \"\\n\".join([f'- \"{m}\" (Similarity: {s:.2f})' for m, s in top_matches]) #m is gold standard sentence and s is similarity score\n",
    "\n",
    "    return f\"\"\"\n",
    "You are acting as a **research compliance auditor**. \n",
    "Your role is to verify whether a given sentence from a scientific document constitutes a **formal acknowledgement** of institutional or financial support.\n",
    "\n",
    "The sentence to review is:\n",
    "\n",
    "**Extracted Sentence:**\n",
    "\"{query_sentence.replace(\"passage: \", \"\")}\"\n",
    "\n",
    "This sentence was selected because it closely matches known acknowledgement phrases:\n",
    "{bullet_matches}\n",
    "\n",
    "### Your task\n",
    "Determine whether the sentence contains a **formal acknowledgement** that refers specifically to **one or more of the following known entities**:\n",
    "- CMCA (Centre for Microscopy Characterisation and Analysis)\n",
    "- The University of Western Australia (UWA)\n",
    "- Microscopy Australia or its nodes\n",
    "- NCRIS or other national research infrastructure programs\n",
    "Only classify as “Yes” if the sentence clearly refers to **these known institutions or programs**\n",
    "\n",
    "### Decision Criteria\n",
    "Classify the sentence as a **formal acknowledgement** only if it clearly refers to:\n",
    "- Use or access of CMCA, UWA microscopy facilities, or Microscopy Australia\n",
    "- Technical or analytical assistance provided by these institutions\n",
    "- Institutional or national infrastructure support (e.g., NCRIS funding or facilities)\n",
    "\n",
    "Strong signals include phrases like:\n",
    "- “use of CMCA facilities”\n",
    "- “supported by Microscopy Australia”\n",
    "- “technical assistance from CMCA”\n",
    "- “funded through the NCRIS program”\n",
    "\n",
    "Do **not** classify it as an acknowledgement if it:\n",
    "- Only thanks individuals without institutional affiliation\n",
    "- Provides generic gratitude with no clear link to facilities, funding, or institutional support\n",
    "\n",
    "### Respond in this exact format:\n",
    "Answer: [Yes or No]  \n",
    "Reason: Explain **why** this sentence qualifies (or not) based on the decision criteria above. Use 1-3 sentences only.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5182e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "#Initialize client\n",
    "client = OpenAI(api_key= \"API\")\n",
    "\n",
    "def verify_acknowledgement(filtered_results):\n",
    "    results = {}\n",
    "\n",
    "    for query, matches in filtered_results.items():\n",
    "        if matches:\n",
    "            prompt = build_prompt(query, matches)\n",
    "\n",
    "            if prompt is None:\n",
    "                continue\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a research auditor.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.0\n",
    "            )\n",
    "\n",
    "            results[query] = response.choices[0].message.content.strip()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and show the result\n",
    "result = verify_acknowledgement(filtered_results)\n",
    "for query, response in result.items():\n",
    "    print(f\"LLM Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b9e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the summarise function that returns the Json:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
