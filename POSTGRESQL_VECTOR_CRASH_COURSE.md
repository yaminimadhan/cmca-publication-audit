# PostgreSQL Vector Database - Complete Crash Course

## ğŸ“š Table of Contents
1. [What is a Vector Database?](#what-is-a-vector-database)
2. [Understanding Embeddings](#understanding-embeddings)
3. [PostgreSQL Fundamentals](#postgresql-fundamentals)
4. [Vector Storage in PostgreSQL](#vector-storage-in-postgresql)
5. [How Vector Similarity Works](#how-vector-similarity-works)
6. [Our Implementation Architecture](#our-implementation-architecture)
7. [Database Schema Deep Dive](#database-schema-deep-dive)
8. [API Architecture](#api-architecture)
9. [Connection Management](#connection-management)
10. [Performance Considerations](#performance-considerations)
11. [Integration Patterns](#integration-patterns)
12. [Troubleshooting Guide](#troubleshooting-guide)

---

## ğŸ¯ What is a Vector Database?

### Traditional vs Vector Databases

**Traditional Database:**
```
Document: "The cat sat on the mat"
Query: "cat" â†’ Exact text match
Result: Finds documents containing "cat"
```

**Vector Database:**
```
Document: "The cat sat on the mat" â†’ [0.1, 0.2, 0.3, ..., 0.384]
Query: "feline" â†’ [0.15, 0.18, 0.32, ..., 0.391]
Result: Finds semantically similar documents based on meaning
```

### Why Vector Databases Matter

1. **Semantic Search**: Find documents by meaning, not just keywords
2. **Similarity Matching**: Discover related content automatically
3. **Machine Learning Integration**: Work with AI models seamlessly
4. **Scalability**: Handle millions of high-dimensional vectors efficiently

### Real-World Applications

- **Document Retrieval**: Find relevant papers, articles, or reports
- **Recommendation Systems**: Suggest similar content to users
- **Question Answering**: Match questions to relevant answers
- **Content Moderation**: Find similar problematic content
- **Knowledge Graphs**: Connect related concepts

---

## ğŸ§  Understanding Embeddings

### What are Embeddings?

Embeddings are numerical representations of text, images, or other data that capture semantic meaning in a high-dimensional space.

### How Embeddings Work

```
Text Input: "machine learning"
â†“
Embedding Model (all-MiniLM-L6-v2)
â†“
Vector Output: [0.1, 0.2, 0.3, ..., 0.384]
```

### Key Properties of Embeddings

1. **Dimensionality**: Fixed size (384 for all-MiniLM-L6-v2)
2. **Semantic Similarity**: Similar texts â†’ Similar vectors
3. **Distance Metrics**: Cosine, Euclidean, Manhattan
4. **Normalization**: Often unit vectors for cosine similarity

### all-MiniLM-L6-v2 Specifics

- **Dimensions**: 384
- **Model Type**: Sentence Transformer
- **Language**: English
- **Use Case**: General-purpose semantic similarity
- **Performance**: Fast inference, good quality

### Embedding Examples

```python
# Similar texts produce similar embeddings
text1 = "The cat is sleeping"
text2 = "The feline is resting"
# These will have high cosine similarity

# Different texts produce different embeddings
text3 = "The dog is running"
# This will have lower similarity to text1/text2
```

---

## ğŸ—„ï¸ PostgreSQL Fundamentals

### What is PostgreSQL?

PostgreSQL is a powerful, open-source relational database management system (RDBMS) that supports:

- **ACID Properties**: Atomicity, Consistency, Isolation, Durability
- **SQL Compliance**: Standard SQL with extensions
- **Extensibility**: Custom data types, functions, operators
- **Performance**: Advanced indexing, query optimization
- **Reliability**: Robust transaction handling

### Key PostgreSQL Concepts

#### 1. Databases and Schemas
```
PostgreSQL Server
â”œâ”€â”€ Database: postgres
â”‚   â””â”€â”€ Schema: public
â”‚       â”œâ”€â”€ Tables
â”‚       â”œâ”€â”€ Views
â”‚       â”œâ”€â”€ Functions
â”‚       â””â”€â”€ Extensions
```

#### 2. Data Types
- **Basic Types**: TEXT, INTEGER, REAL, BOOLEAN, TIMESTAMP
- **Advanced Types**: JSONB, ARRAY, UUID
- **Custom Types**: VECTOR (with pgvector extension)

#### 3. Indexing
- **B-tree**: Default for most queries
- **GIN**: Generalized Inverted Index (for arrays, JSONB)
- **GiST**: Generalized Search Tree (for geometric data)
- **HNSW**: Hierarchical Navigable Small World (for vectors)

### Connection Architecture

```
Application
    â†“ (psycopg2)
PostgreSQL Server
    â”œâ”€â”€ Connection Pool
    â”œâ”€â”€ Query Parser
    â”œâ”€â”€ Query Planner
    â”œâ”€â”€ Query Executor
    â””â”€â”€ Storage Engine
```

---

## ğŸ”¢ Vector Storage in PostgreSQL

### Our Implementation: PostgreSQL Arrays

Since pgvector extension isn't available in your PostgreSQL installation, we use PostgreSQL arrays:

```sql
-- Vector stored as REAL array
embedding REAL[]  -- Example: {0.1, 0.2, 0.3, 0.4, 0.5}
```

### Array Operations

```sql
-- Insert vector
INSERT INTO vector_documents (embedding) 
VALUES (ARRAY[0.1, 0.2, 0.3, 0.4, 0.5]);

-- Query vector
SELECT embedding FROM vector_documents 
WHERE id = 'doc1';

-- Array length
SELECT array_length(embedding, 1) as dimensions 
FROM vector_documents;
```

### Alternative: pgvector Extension

If pgvector were available, we'd use:

```sql
-- Enable extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Create vector column
embedding VECTOR(384)

-- Vector operations
SELECT embedding <-> query_vector as distance
FROM vector_documents
ORDER BY distance;
```

### Storage Comparison

| Method | Pros | Cons |
|--------|------|------|
| PostgreSQL Arrays | Universal compatibility | Manual similarity calculation |
| pgvector Extension | Built-in vector ops | Requires extension installation |

---

## ğŸ“ How Vector Similarity Works

### Distance Metrics

#### 1. Cosine Similarity
```python
def cosine_similarity(a, b):
    dot_product = sum(x * y for x, y in zip(a, b))
    magnitude_a = sqrt(sum(x * x for x in a))
    magnitude_b = sqrt(sum(x * x for x in b))
    return dot_product / (magnitude_a * magnitude_b)
```

**Range**: -1 to 1 (1 = identical, 0 = orthogonal, -1 = opposite)

#### 2. Cosine Distance
```python
cosine_distance = 1 - cosine_similarity
```

**Range**: 0 to 2 (0 = identical, 1 = orthogonal, 2 = opposite)

#### 3. Euclidean Distance
```python
def euclidean_distance(a, b):
    return sqrt(sum((x - y) ** 2 for x, y in zip(a, b)))
```

### Similarity Search Process

```
1. Query Vector: [0.1, 0.2, 0.3, 0.4, 0.5]
2. Retrieve All Vectors: [doc1_vec, doc2_vec, doc3_vec, ...]
3. Calculate Similarities: [0.95, 0.87, 0.23, ...]
4. Sort by Similarity: [doc1_vec, doc2_vec, doc3_vec, ...]
5. Return Top K Results
```

### Performance Considerations

- **Brute Force**: O(n) for n documents
- **Indexed Search**: O(log n) with proper indexing
- **Approximate Search**: Trade accuracy for speed

---

## ğŸ—ï¸ Our Implementation Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Embedding     â”‚    â”‚   Vector DB     â”‚    â”‚   Search        â”‚
â”‚   Generation    â”‚â”€â”€â”€â–¶â”‚   (PostgreSQL)  â”‚â—€â”€â”€â”€â”‚   Application   â”‚
â”‚   Team          â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ all-MiniLM-L6-  â”‚    â”‚ FastAPI Server  â”‚    â”‚ Vector Similarityâ”‚
â”‚ v2 Model        â”‚    â”‚ (pgvector_api)  â”‚    â”‚ Search Results  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Responsibilities

#### 1. Embedding Generation Team
- **Input**: Raw documents (text, PDFs, etc.)
- **Process**: Generate embeddings using all-MiniLM-L6-v2
- **Output**: Documents + embeddings + metadata
- **Format**: JSON with 384-dimensional vectors

#### 2. Vector Database (Your Role)
- **Input**: Precomputed embeddings
- **Process**: Store, index, and retrieve vectors
- **Output**: Similarity search results
- **Technology**: PostgreSQL + FastAPI

#### 3. Search Application
- **Input**: Query text
- **Process**: Generate query embedding, search database
- **Output**: Ranked similar documents

### Data Flow

```
1. Documents â†’ Embedding Model â†’ Embeddings
2. Embeddings â†’ API â†’ PostgreSQL Storage
3. Query â†’ Embedding Model â†’ Query Vector
4. Query Vector â†’ Database â†’ Similar Documents
```

---

## ğŸ—ƒï¸ Database Schema Deep Dive

### Table: vector_collections

```sql
CREATE TABLE vector_collections (
    collection_name TEXT PRIMARY KEY,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Purpose**: Organize documents into logical groups
**Example Data**:
```
collection_name    | created_at
------------------|------------------
research_papers   | 2025-01-03 10:30:00
legal_documents   | 2025-01-03 11:15:00
news_articles     | 2025-01-03 12:00:00
```

### Table: vector_documents

```sql
CREATE TABLE vector_documents (
    id TEXT PRIMARY KEY,
    collection_name TEXT REFERENCES vector_collections(collection_name),
    document TEXT,
    metadata JSONB,
    embedding REAL[],
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Purpose**: Store documents with their vector embeddings
**Example Data**:
```
id      | collection_name | document           | metadata                    | embedding
--------|-----------------|--------------------|----------------------------|------------------
doc_1   | research_papers| "Machine learning..."| {"source": "arxiv", "year": 2024} | {0.1, 0.2, ...}
doc_2   | research_papers| "Deep learning..."  | {"source": "arxiv", "year": 2024} | {0.3, 0.4, ...}
```

### Indexes

```sql
-- Collection lookup
CREATE INDEX idx_vector_documents_collection 
ON vector_documents (collection_name);

-- Metadata queries
CREATE INDEX idx_vector_documents_metadata 
ON vector_documents USING GIN (metadata);

-- Embedding similarity (if pgvector available)
CREATE INDEX idx_vector_documents_embedding 
ON vector_documents USING GIN (embedding);
```

### Relationships

```
vector_collections (1) â†â†’ (many) vector_documents
     collection_name â†â†’ collection_name
```

---

## ğŸŒ API Architecture

### FastAPI Server (pgvector_api.py)

#### Core Components

```python
from fastapi import FastAPI
from pgvector_db import VectorDB

app = FastAPI(title="PostgreSQL Vector Database API")
db = VectorDB()  # Database connection
```

#### Endpoint Structure

```
POST /collections          # Create collection
GET  /collections          # List collections
POST /items                # Add documents
POST /search/vector        # Vector similarity search
```

#### Request/Response Models

```python
# Add documents
{
    "collection_name": "research_papers",
    "ids": ["doc1", "doc2"],
    "documents": ["Document 1 content", "Document 2 content"],
    "metadatas": [{"source": "arxiv"}, {"source": "arxiv"}],
    "embeddings": [[0.1, 0.2, ...], [0.3, 0.4, ...]]
}

# Search response
{
    "ids": ["doc1", "doc3"],
    "documents": ["Document 1", "Document 3"],
    "metadatas": [{"source": "arxiv"}, {"source": "arxiv"}],
    "distances": [0.05, 0.12],
    "similarities": [0.95, 0.88]
}
```

### Database Layer (pgvector_db.py)

#### Connection Management

```python
class VectorDB:
    def __init__(self):
        self.conn_string = os.getenv('DATABASE_URL')
        self._ensure_extension()  # Check pgvector availability
    
    def _get_connection(self):
        return psycopg2.connect(self.conn_string)
```

#### Core Operations

```python
def create_collection(self, name):
    # Create collection table
    
def add_with_embeddings(self, name, ids, documents, metadatas, embeddings):
    # Insert documents with vectors
    
def query_by_vector(self, name, query_embeddings, n_results):
    # Perform similarity search
```

---

## ğŸ”Œ Connection Management

### Environment Variables

```bash
# Required
DATABASE_URL=postgresql://postgres:password@localhost:5432/postgres

# Optional
API_HOST=0.0.0.0
API_PORT=8000
```

### Connection String Format

```
postgresql://[user]:[password]@[host]:[port]/[database]
```

**Components**:
- **user**: postgres
- **password**: password
- **host**: localhost
- **port**: 5432
- **database**: postgres

### Connection Pooling

```python
# psycopg2 connection pooling
from psycopg2 import pool

connection_pool = psycopg2.pool.SimpleConnectionPool(
    minconn=1,
    maxconn=20,
    dsn=os.getenv('DATABASE_URL')
)
```

### Error Handling

```python
try:
    conn = psycopg2.connect(dsn)
    # Database operations
except psycopg2.OperationalError as e:
    # Connection failed
    print(f"Database connection failed: {e}")
except psycopg2.IntegrityError as e:
    # Constraint violation
    print(f"Data integrity error: {e}")
```

---

## âš¡ Performance Considerations

### Database Performance

#### 1. Indexing Strategy
```sql
-- Collection-based queries
CREATE INDEX idx_collection ON vector_documents (collection_name);

-- Metadata filtering
CREATE INDEX idx_metadata ON vector_documents USING GIN (metadata);

-- Vector similarity (with pgvector)
CREATE INDEX idx_vector ON vector_documents USING hnsw (embedding);
```

#### 2. Query Optimization
```sql
-- Efficient: Use indexes
SELECT * FROM vector_documents 
WHERE collection_name = 'research_papers';

-- Inefficient: Full table scan
SELECT * FROM vector_documents 
WHERE document LIKE '%machine learning%';
```

#### 3. Batch Operations
```python
# Efficient: Batch insert
INSERT INTO vector_documents (id, collection_name, document, embedding)
VALUES 
    ('doc1', 'collection1', 'doc1 content', ARRAY[0.1, 0.2, 0.3]),
    ('doc2', 'collection1', 'doc2 content', ARRAY[0.4, 0.5, 0.6]),
    ('doc3', 'collection1', 'doc3 content', ARRAY[0.7, 0.8, 0.9]);
```

### API Performance

#### 1. Connection Reuse
```python
# Reuse connections
with db._get_connection() as conn:
    # Multiple operations
```

#### 2. Async Operations
```python
# FastAPI async support
@app.post("/items")
async def add_items(data: DocumentData):
    # Async database operations
```

#### 3. Response Caching
```python
# Cache frequent queries
from functools import lru_cache

@lru_cache(maxsize=100)
def get_collection_info(collection_name):
    # Cached collection metadata
```

### Scaling Considerations

#### Horizontal Scaling
- **Read Replicas**: Multiple PostgreSQL instances
- **Load Balancing**: Distribute API requests
- **Sharding**: Partition data by collection

#### Vertical Scaling
- **Memory**: Increase shared_buffers
- **CPU**: More cores for parallel queries
- **Storage**: SSD for faster I/O

---

## ğŸ”— Integration Patterns

### 1. Embedding Generation Integration

```python
# Your team's embedding generation
def generate_embeddings(documents):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(documents)
    return embeddings

# Send to vector database
def store_embeddings(collection_name, documents, embeddings):
    data = {
        "collection_name": collection_name,
        "ids": [f"doc_{i}" for i in range(len(documents))],
        "documents": documents,
        "metadatas": [{"source": "generated"} for _ in documents],
        "embeddings": embeddings.tolist()
    }
    
    response = requests.post("http://localhost:8000/items", json=data)
    return response.json()
```

### 2. Search Integration

```python
# Query processing
def search_documents(query_text, collection_name, n_results=10):
    # Generate query embedding
    model = SentenceTransformer('all-MiniLM-L6-v2')
    query_embedding = model.encode([query_text])[0]
    
    # Search vector database
    search_data = {
        "collection_name": collection_name,
        "query_embedding": query_embedding.tolist(),
        "n_results": n_results
    }
    
    response = requests.post("http://localhost:8000/search/vector", json=search_data)
    return response.json()
```

### 3. Batch Processing

```python
# Large dataset import
def import_large_dataset(documents, batch_size=1000):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]
        embeddings = model.encode(batch)
        
        store_embeddings(
            collection_name="large_dataset",
            documents=batch,
            embeddings=embeddings
        )
        
        print(f"Processed {i + len(batch)} documents")
```

### 4. Real-time Updates

```python
# Streaming updates
def stream_document_updates(document_stream):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    for document in document_stream:
        embedding = model.encode([document])[0]
        
        data = {
            "collection_name": "streaming",
            "ids": [document.id],
            "documents": [document.content],
            "metadatas": [document.metadata],
            "embeddings": [embedding.tolist()]
        }
        
        requests.post("http://localhost:8000/items", json=data)
```

---

## ğŸ› ï¸ Troubleshooting Guide

### Common Issues and Solutions

#### 1. Connection Problems

**Error**: `psycopg2.OperationalError: connection to server failed`

**Solutions**:
```bash
# Check PostgreSQL service
Get-Service | Where-Object {$_.DisplayName -like "*PostgreSQL*"}

# Test connection
python -c "import psycopg2; psycopg2.connect('postgresql://postgres:password@localhost:5432/postgres')"

# Reset password
# Check PostgreSQL service is running
```

#### 2. Authentication Issues

**Error**: `password authentication failed for user "postgres"`

**Solutions**:
```bash
# Set environment variable
$env:DATABASE_URL="postgresql://postgres:password@localhost:5432/postgres"

# Run password reset
# Check PostgreSQL service is running
```

#### 3. Vector Operations

**Error**: `operator does not exist: real[] <-> numeric[]`

**Cause**: pgvector extension not available

**Solution**: Use our PostgreSQL array implementation
```python
# Manual similarity calculation
def cosine_similarity(a, b):
    dot_product = sum(x * y for x, y in zip(a, b))
    magnitude_a = sqrt(sum(x * x for x in a))
    magnitude_b = sqrt(sum(x * x for x in b))
    return dot_product / (magnitude_a * magnitude_b)
```

#### 4. Performance Issues

**Problem**: Slow vector similarity search

**Solutions**:
```sql
-- Add indexes
CREATE INDEX idx_collection ON vector_documents (collection_name);
CREATE INDEX idx_metadata ON vector_documents USING GIN (metadata);

-- Limit result set
SELECT * FROM vector_documents 
WHERE collection_name = 'research_papers'
LIMIT 100;
```

#### 5. Memory Issues

**Error**: `out of memory`

**Solutions**:
```python
# Process in batches
def process_large_dataset(documents, batch_size=100):
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i + batch_size]
        process_batch(batch)
```

### Debugging Tools

#### 1. Database Monitoring
```sql
-- Check table sizes
SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables 
WHERE schemaname = 'public';

-- Check query performance
EXPLAIN ANALYZE SELECT * FROM vector_documents 
WHERE collection_name = 'research_papers';
```

#### 2. API Monitoring
```python
# Add logging
import logging
logging.basicConfig(level=logging.INFO)

# Monitor response times
import time
start_time = time.time()
# API call
end_time = time.time()
print(f"Request took {end_time - start_time:.2f} seconds")
```

#### 3. Connection Monitoring
```python
# Check active connections
SELECT 
    datname,
    usename,
    application_name,
    client_addr,
    state
FROM pg_stat_activity 
WHERE datname = 'postgres';
```

---

## ğŸ“Š Monitoring and Maintenance

### Health Checks

#### 1. Database Health
```python
def check_database_health():
    try:
        conn = psycopg2.connect(os.getenv('DATABASE_URL'))
        with conn.cursor() as cur:
            cur.execute("SELECT 1")
            return True
    except Exception as e:
        print(f"Database health check failed: {e}")
        return False
```

#### 2. API Health
```python
@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "database": check_database_health(),
        "timestamp": datetime.now().isoformat()
    }
```

### Backup Strategy

#### 1. Database Backup
```bash
# Full backup
pg_dump -U postgres -h localhost postgres > backup.sql

# Restore
psql -U postgres -h localhost postgres < backup.sql
```

#### 2. Data Export
```python
def export_collection(collection_name):
    conn = psycopg2.connect(os.getenv('DATABASE_URL'))
    with conn.cursor() as cur:
        cur.execute("""
            SELECT id, document, metadata, embedding
            FROM vector_documents
            WHERE collection_name = %s
        """, (collection_name,))
        
        return cur.fetchall()
```

### Performance Monitoring

#### 1. Query Performance
```sql
-- Slow queries
SELECT 
    query,
    calls,
    total_time,
    mean_time
FROM pg_stat_statements 
ORDER BY total_time DESC 
LIMIT 10;
```

#### 2. Index Usage
```sql
-- Index usage statistics
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes;
```

---

## ğŸ¯ Best Practices

### 1. Data Management
- **Consistent Naming**: Use clear collection and document names
- **Metadata Standards**: Define consistent metadata schemas
- **Version Control**: Track document versions and updates
- **Data Validation**: Validate embeddings before storage

### 2. Performance Optimization
- **Batch Operations**: Process multiple documents together
- **Index Strategy**: Create appropriate indexes for your queries
- **Connection Pooling**: Reuse database connections
- **Caching**: Cache frequent queries and results

### 3. Security
- **Access Control**: Limit database access to necessary users
- **Input Validation**: Validate all API inputs
- **Error Handling**: Don't expose sensitive information in errors
- **Audit Logging**: Log important operations

### 4. Monitoring
- **Health Checks**: Regular system health monitoring
- **Performance Metrics**: Track response times and throughput
- **Error Tracking**: Monitor and alert on errors
- **Capacity Planning**: Monitor storage and memory usage

---

## ğŸš€ Future Enhancements

### 1. pgvector Migration
When pgvector becomes available:
```sql
-- Enable extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Migrate to vector type
ALTER TABLE vector_documents 
ALTER COLUMN embedding TYPE VECTOR(384);

-- Add vector index
CREATE INDEX ON vector_documents 
USING hnsw (embedding vector_cosine_ops);
```

### 2. Advanced Features
- **Hybrid Search**: Combine vector and keyword search
- **Multi-modal**: Support images and other data types
- **Real-time Updates**: Streaming document updates
- **Distributed Search**: Multi-node vector search

### 3. Integration Improvements
- **GraphQL API**: More flexible query interface
- **WebSocket Support**: Real-time updates
- **Batch Processing**: Large dataset import tools
- **Analytics Dashboard**: Usage and performance metrics

---

This crash course provides a comprehensive understanding of PostgreSQL vector databases, from basic concepts to advanced implementation details. Use this as a reference guide for understanding, troubleshooting, and extending your vector database system.
